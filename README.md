# Artificial_Neural_Networks
This repository was made to demonstrate the codes associated with my graduation's final project: Comparison of Data Regularization Methods applied to Artificial Neural Networks to Predict the Water Outlet Temperature in an Industrial Cooling Tower.


Abstract:

The growing development and application of artificial intelligence in processes on various industries promotes the need to increasingly study and adapt the use of these tools to processes of the chemical industry. Among these tools, are the Artificial Neural Networks, which generalization capacities have the potential to, under the correct circumstances of the database and training, deliver results very close to reality. In that regard, this study has, as objective, testing the regularization techniques usually employed in the training of neural networks, which aim at avoiding the problems of overfitting associated to the model, having as study base the analysis of a water industrial cooling tower. The tested techniques were the L1, L2 and Dropout regularizations, which were applied to networks that were trained using various configurations, along with several values of hyperparameters. By analyzing the results of the performance metrics mean absolute error (MAE) and root mean square error (RMSE), as well as the behaviour of the learning curves resulted from the networks’ training under various conditions, it was possible to verify which network architectures obtained the best prediction results without data regularization, for the cases of one and two hidden layers. The L2 technique with λ = 0,01 in the case of the neural network with 5 internal nodes was the only case in which the regularization resulted in values of MAE and RMSE smaller than the network with no regularization. From this result, by using exploratory analysis, it was investigated the value of λ (L2) that provided the smallest MAE and RMSE for this network, obtaining the value of λ = 0,01597. However, it was verified that, in general, the regularization techniques did not deliver significant improvements in the generalization capacity of the neural networks and did not demonstrate, in the learning curves, a decrease of the overfitting effects, even if present in small intensity.
